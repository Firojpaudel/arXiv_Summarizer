{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10408342,"sourceType":"datasetVersion","datasetId":6449972}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## The notebook implementation of Research paper summarizer:\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install transformers datasets torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"facebook/bart-large-cnn\"\ntokenizer = AutoTokenizer.from_pretrained(model_name,clean_up_tokenization_spaces= True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:42:45.266647Z","iopub.execute_input":"2025-01-09T07:42:45.266949Z","iopub.status.idle":"2025-01-09T07:43:03.474207Z","shell.execute_reply.started":"2025-01-09T07:42:45.266925Z","shell.execute_reply":"2025-01-09T07:43:03.473579Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a38eeb722d94b188da0c09d75c15b13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da840393d7ee4f9c95eff5049fd89611"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4969e184741844a18eddb9da5a30788a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0088a4a20eee48b0b63a2fb5af33d671"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3076a4420b8e4f1fb405aeb6c3eb8b7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f7bfb62ccab4ff2b5eaf8f87e7b64e8"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:43:03.475476Z","iopub.execute_input":"2025-01-09T07:43:03.475802Z","iopub.status.idle":"2025-01-09T07:43:05.054883Z","shell.execute_reply.started":"2025-01-09T07:43:03.475783Z","shell.execute_reply":"2025-01-09T07:43:05.053840Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"dataset = load_dataset(\"scientific_papers\", \"arxiv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T01:24:18.023536Z","iopub.execute_input":"2025-01-09T01:24:18.023858Z","iopub.status.idle":"2025-01-09T01:29:29.139477Z","shell.execute_reply.started":"2025-01-09T01:24:18.023838Z","shell.execute_reply":"2025-01-09T01:29:29.137217Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e30a59ba6c5a43f085e31bd30f3288f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scientific_papers.py:   0%|          | 0.00/5.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bd8a880c35d4ed8885b47e7e1bbc83a"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for scientific_papers contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/scientific_papers.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.62G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b2a792db646443db31d1348cfe4c881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/880M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"532b4d30223d42e79fc16d2f2fa9f8bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/203037 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb17da788f444983bff1ae0f34077d3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/6436 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6424082b65d4123985abfd34963bdbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/6440 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba9c94800a874eb19e5950bc3f72f755"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"print(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T01:29:29.141256Z","iopub.execute_input":"2025-01-09T01:29:29.141515Z","iopub.status.idle":"2025-01-09T01:29:29.148058Z","shell.execute_reply.started":"2025-01-09T01:29:29.141491Z","shell.execute_reply":"2025-01-09T01:29:29.147412Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'abstract', 'section_names'],\n        num_rows: 203037\n    })\n    validation: Dataset({\n        features: ['article', 'abstract', 'section_names'],\n        num_rows: 6436\n    })\n    test: Dataset({\n        features: ['article', 'abstract', 'section_names'],\n        num_rows: 6440\n    })\n})\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def preprocess_func(text):\n    inputs = text['article']\n    targets= text['abstract']\n    model_inputs = tokenizer(inputs, max_length = 1024, truncation= True, padding= 'max_length')\n    labels = tokenizer(targets, max_length= 256, truncation= True, padding= 'max_length')\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset= dataset.map(preprocess_func, batched= True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T01:29:29.149099Z","iopub.execute_input":"2025-01-09T01:29:29.149416Z","iopub.status.idle":"2025-01-09T02:06:22.889440Z","shell.execute_reply.started":"2025-01-09T01:29:29.149383Z","shell.execute_reply":"2025-01-09T02:06:22.888519Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/203037 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d858a7e030fb4eb3b73b3a434f7060d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6436 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fdab53dd7c841b18d648398a9a9bbf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6440 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25861cc370bd479885c19c8d942baf6c"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# #@ Saving the mapped dataset so that it becomes effecient if training error occours \n\n# tokenized_dataset.save_to_disk(\"mapped_dataset_dir\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T02:14:07.310597Z","iopub.execute_input":"2025-01-09T02:14:07.310911Z","iopub.status.idle":"2025-01-09T02:14:52.910857Z","shell.execute_reply.started":"2025-01-09T02:14:07.310887Z","shell.execute_reply":"2025-01-09T02:14:52.909850Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/18 shards):   0%|          | 0/203037 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c1acd2999734ef98fd3a883f80ade2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6436 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dbbe15891b4403ab5cd10d1e31201b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6440 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36dcf989be71434f92c15f095735fab4"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# !zip -r compressed_mapped.zip mapped_dataset_dir","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Okay, now I'm not mapping it again. Instead, I'll unzip the mapped_value and run from this. ","metadata":{}},{"cell_type":"code","source":"from datasets import load_from_disk\n\nmapped_df= load_from_disk('/kaggle/input/compressed-mapped-files/mapped_dataset_dir')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:43:05.056432Z","iopub.execute_input":"2025-01-09T07:43:05.057135Z","iopub.status.idle":"2025-01-09T07:43:05.507070Z","shell.execute_reply.started":"2025-01-09T07:43:05.057097Z","shell.execute_reply":"2025-01-09T07:43:05.506088Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading dataset from disk:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1fde665a5fd4303ac5f50357df4331e"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args= TrainingArguments(\n    output_dir= \"./results\",\n    eval_strategy= \"epoch\", \n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    learning_rate= 2e-5, \n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    num_train_epochs= 3, \n    weight_decay = 0.01, \n    logging_dir=\"./logs\",\n    fp16 = True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:43:27.641419Z","iopub.execute_input":"2025-01-09T07:43:27.641884Z","iopub.status.idle":"2025-01-09T07:43:43.341128Z","shell.execute_reply.started":"2025-01-09T07:43:27.641842Z","shell.execute_reply":"2025-01-09T07:43:43.340349Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# small_train = tokenized_dataset[\"train\"].select(range(10000))\n# small_eval = tokenized_dataset[\"validation\"].select(range(4000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T02:07:13.240529Z","iopub.execute_input":"2025-01-09T02:07:13.240814Z","iopub.status.idle":"2025-01-09T02:07:13.249618Z","shell.execute_reply.started":"2025-01-09T02:07:13.240793Z","shell.execute_reply":"2025-01-09T02:07:13.248926Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"small_train = mapped_df[\"train\"].select(range(10000))\nsmall_eval = mapped_df[\"validation\"].select(range(2000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:43:43.342274Z","iopub.execute_input":"2025-01-09T07:43:43.342905Z","iopub.status.idle":"2025-01-09T07:43:43.352164Z","shell.execute_reply.started":"2025-01-09T07:43:43.342869Z","shell.execute_reply":"2025-01-09T07:43:43.351311Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train,\n    eval_dataset=small_eval,\n    tokenizer=tokenizer,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:44:14.800623Z","iopub.execute_input":"2025-01-09T07:44:14.800951Z","iopub.status.idle":"2025-01-09T07:44:17.646083Z","shell.execute_reply.started":"2025-01-09T07:44:14.800930Z","shell.execute_reply":"2025-01-09T07:44:17.645433Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_api_key\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:44:17.647240Z","iopub.execute_input":"2025-01-09T07:44:17.647562Z","iopub.status.idle":"2025-01-09T07:44:17.783366Z","shell.execute_reply.started":"2025-01-09T07:44:17.647531Z","shell.execute_reply":"2025-01-09T07:44:17.782757Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import wandb\nwandb.login(key= secret_value_0)\nwandb.init(project=\"Researchpaper_summarize\", name=\"wtf\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:44:18.557149Z","iopub.execute_input":"2025-01-09T07:44:18.557424Z","iopub.status.idle":"2025-01-09T07:44:31.502648Z","shell.execute_reply.started":"2025-01-09T07:44:18.557403Z","shell.execute_reply":"2025-01-09T07:44:31.501832Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfirojpaudel\u001b[0m (\u001b[33mfirojpaudel-madan-bhandari-memorial-college\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250109_074425-k0weyuq7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize/runs/k0weyuq7' target=\"_blank\">wtf</a></strong> to <a href='https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize' target=\"_blank\">https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize/runs/k0weyuq7' target=\"_blank\">https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize/runs/k0weyuq7</a>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize/runs/k0weyuq7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7b57c5202320>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:44:31.503917Z","iopub.execute_input":"2025-01-09T07:44:31.504300Z","iopub.status.idle":"2025-01-09T10:31:28.710471Z","shell.execute_reply.started":"2025-01-09T07:44:31.504266Z","shell.execute_reply":"2025-01-09T10:31:28.709585Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 2:46:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.141400</td>\n      <td>1.818125</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.867500</td>\n      <td>1.777878</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.785800</td>\n      <td>1.769484</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1875, training_loss=1.8924777669270834, metrics={'train_runtime': 10015.5745, 'train_samples_per_second': 2.995, 'train_steps_per_second': 0.187, 'total_flos': 6.501313806336e+16, 'train_loss': 1.8924777669270834, 'epoch': 3.0})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"import shutil\nimport os\nlogs_dir = '/kaggle/working/ft_summ'\ntry:\n    if os.path.exists(logs_dir):\n        shutil.rmtree(logs_dir)  # Remove the directory and its contents\n        print(f\"Directory '{logs_dir}' removed successfully.\")\n    else:\n        print(f\"Directory '{logs_dir}' does not exist.\")\nexcept OSError as e:\n    print(f\"Error removing directory '{logs_dir}': {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:36:20.678009Z","iopub.execute_input":"2025-01-09T10:36:20.678318Z","iopub.status.idle":"2025-01-09T10:36:21.000357Z","shell.execute_reply.started":"2025-01-09T10:36:20.678291Z","shell.execute_reply":"2025-01-09T10:36:20.999611Z"}},"outputs":[{"name":"stdout","text":"Directory '/kaggle/working/ft_summ' removed successfully.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"##@ saving the finetuned model:\ntrainer.save_model(\"./fine_tuned_bart\")\ntokenizer.save_pretrained(\"./fine_tuned_bart\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:36:55.817087Z","iopub.execute_input":"2025-01-09T10:36:55.817442Z","iopub.status.idle":"2025-01-09T10:36:58.768844Z","shell.execute_reply.started":"2025-01-09T10:36:55.817415Z","shell.execute_reply":"2025-01-09T10:36:58.767959Z"}},"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('./fine_tuned_bart/tokenizer_config.json',\n './fine_tuned_bart/special_tokens_map.json',\n './fine_tuned_bart/vocab.json',\n './fine_tuned_bart/merges.txt',\n './fine_tuned_bart/added_tokens.json',\n './fine_tuned_bart/tokenizer.json')"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import os\n\nprint(os.listdir(\"./fine_tuned_bart\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:38:40.317897Z","iopub.execute_input":"2025-01-09T10:38:40.318223Z","iopub.status.idle":"2025-01-09T10:38:40.324335Z","shell.execute_reply.started":"2025-01-09T10:38:40.318195Z","shell.execute_reply":"2025-01-09T10:38:40.323563Z"}},"outputs":[{"name":"stdout","text":"['model.safetensors', 'training_args.bin', 'special_tokens_map.json', 'vocab.json', 'tokenizer_config.json', 'tokenizer.json', 'config.json', 'generation_config.json', 'merges.txt']\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"all the weights are there in model.safetensors and training arguments in training_args.bins","metadata":{}},{"cell_type":"code","source":"#@ Testing once before downloading the model \n\nmodel= AutoModelForSeq2SeqLM.from_pretrained('/kaggle/working/fine_tuned_bart')\ntokenizer= AutoTokenizer.from_pretrained('/kaggle/working/fine_tuned_bart')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:45:14.919999Z","iopub.execute_input":"2025-01-09T10:45:14.920279Z","iopub.status.idle":"2025-01-09T10:45:15.937024Z","shell.execute_reply.started":"2025-01-09T10:45:14.920258Z","shell.execute_reply":"2025-01-09T10:45:15.936090Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def summarize(testing):\n    #@ Tokenizng the inp text to test in the finetuned model\n    inputs = tokenizer(testing, max_length= 1024, truncation= True, return_tensors='pt')\n\n    #@ Generating the summary\n    summary_enc = model.generate(inputs['input_ids'], max_length= 500, min_length= 250, length_penalty= 2.0, num_beams=4, early_stopping= True)\n\n    #@ Decoding the generated summary\n    summary_fin = tokenizer.decode(summary_enc[0], skip_special_tokens= True)\n\n    return summary_fin","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:45:15.938148Z","iopub.execute_input":"2025-01-09T10:45:15.938455Z","iopub.status.idle":"2025-01-09T10:45:15.943762Z","shell.execute_reply.started":"2025-01-09T10:45:15.938426Z","shell.execute_reply":"2025-01-09T10:45:15.943060Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"input = '''\nBERT: Pre-training of Deep Bidirectional Transformers for\n Language Understanding\n Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\n Google AI Language\n jacobdevlin,mingweichang,kentonl,kristout@google.com\n Abstract\n arXiv:1810.04805v2  [cs.CL]  24 May 2019\n We introduce a new language representa\ntion model called BERT, which stands for\n Bidirectional Encoder Representations from\n Transformers. Unlike recent language repre\nsentation models (Peters et al., 2018a; Rad\nford et al., 2018), BERT is designed to pre\ntrain deep bidirectional representations from\n unlabeled text by jointly conditioning on both\n left and right context in all layers. As a re\nsult, the pre-trained BERT model can be fine\ntuned with just one additional output layer\n to create state-of-the-art models for a wide\n range of tasks, such as question answering and\n language inference, without substantial task\nspecific architecture modifications.\n BERT is conceptually simple and empirically\n powerful. It obtains new state-of-the-art re\nsults on eleven natural language processing\n tasks, including pushing the GLUE score to\n 80.5% (7.7% point absolute improvement),\n MultiNLI accuracy to 86.7% (4.6% absolute\n improvement), SQuAD v1.1 question answer\ning Test F1 to 93.2 (1.5 point absolute im\nprovement) and SQuAD v2.0 Test F1 to 83.1\n (5.1 point absolute improvement).\n 1 Introduction\n Language model pre-training has been shown to\n be effective for improving many natural language\n processing tasks (Dai and Le, 2015; Peters et al.,\n 2018a; Radford et al., 2018; Howard and Ruder,\n 2018). These include sentence-level tasks such as\n natural language inference (Bowman et al., 2015;\n Williams et al., 2018) and paraphrasing (Dolan\n and Brockett, 2005), which aim to predict the re\nlationships between sentences by analyzing them\n holistically, as well as token-level tasks such as\n named entity recognition and question answering,\n where models are required to produce fine-grained\n output at the token level (Tjong Kim Sang and\n De Meulder, 2003; Rajpurkar et al., 2016).\n There are two existing strategies for apply\ning pre-trained language representations to down\nstream tasks: feature-based and fine-tuning. The\n feature-based approach, such as ELMo (Peters\n et al., 2018a), uses task-specific architectures that\n include the pre-trained representations as addi\ntional features. The fine-tuning approach, such as\n the Generative Pre-trained Transformer (OpenAI\n GPT) (Radford et al., 2018), introduces minimal\n task-specific parameters, and is trained on the\n downstream tasks by simply fine-tuning all pre\ntrained parameters. The two approaches share the\n sameobjective function during pre-training, where\n they use unidirectional language models to learn\n general language representations.\n We argue that current techniques restrict the\n power of the pre-trained representations, espe\ncially for the fine-tuning approaches. The ma\njor limitation is that standard language models are\n unidirectional, and this limits the choice of archi\ntectures that can be used during pre-training. For\n example, in OpenAIGPT,theauthors use aleft-to\nright architecture, where every token can only at\ntend to previous tokens in the self-attention layers\n of the Transformer (Vaswani et al., 2017). Such re\nstrictions are sub-optimal for sentence-level tasks,\n and could be very harmful when applying fine\ntuning based approaches to token-level tasks such\n as question answering, where it is crucial to incor\nporate context from both directions.\n In this paper, we improve the fine-tuning based\n approaches by proposing BERT: Bidirectional\n Encoder Representations from Transformers.\n BERT alleviates the previously mentioned unidi\nrectionality constraint by using a “masked lan\nguage model” (MLM) pre-training objective, in\nspired by the Cloze task (Taylor, 1953). The\n masked language model randomly masks some of\n the tokens from the input, and the objective is to\n predict the original vocabulary id of the masked\nword based only on its context. Unlike left-to\nright language model pre-training, the MLM ob\njective enables the representation to fuse the left\n and the right context, which allows us to pre\ntrain a deep bidirectional Transformer. In addi\ntion to the masked language model, we also use\n a “next sentence prediction” task that jointly pre\ntrains text-pair representations. The contributions\n of our paper are as follows:\n • Wedemonstrate the importance of bidirectional\n pre-training for language representations. Un\nlike Radford et al. (2018), which uses unidirec\ntional language models for pre-training, BERT\n uses masked language models to enable pre\ntrained deep bidirectional representations. This\n is also in contrast to Peters et al. (2018a), which\n uses a shallow concatenation of independently\n trained left-to-right and right-to-left LMs.\n • Weshowthatpre-trained representations reduce\n the need for many heavily-engineered task\nspecific architectures. BERT is the first fine\ntuning based representation model that achieves\n state-of-the-art performance on a large suite\n of sentence-level and token-level tasks, outper\nforming many task-specific architectures.\n • BERT advances the state of the art for eleven\n NLP tasks. The code and pre-trained mod\nels are available at https://github.com/\n google-research/bert.\n 2 Related Work\n There is a long history of pre-training general lan\nguage representations, and we briefly review the\n most widely-used approaches in this section.\n 2.1 Unsupervised Feature-based Approaches\n Learning widely applicable representations of\n words has been an active area of research for\n decades, including non-neural (Brown et al., 1992;\n Ando and Zhang, 2005; Blitzer et al., 2006) and\n neural (Mikolov et al., 2013; Pennington et al.,\n 2014) methods. Pre-trained word embeddings\n are an integral part of modern NLP systems, of\nfering significant improvements over embeddings\n learned from scratch (Turian et al., 2010). To pre\ntrain word embedding vectors, left-to-right lan\nguage modeling objectives have been used (Mnih\n and Hinton, 2009), as well as objectives to dis\ncriminate correct from incorrect words in left and\n right context (Mikolov et al., 2013).\n These approaches have been generalized to\n coarser granularities, such as sentence embed\ndings (Kiros et al., 2015; Logeswaran and Lee,\n 2018) or paragraph embeddings (Le and Mikolov,\n 2014). To train sentence representations, prior\n work has used objectives to rank candidate next\n sentences (Jernite et al., 2017; Logeswaran and\n Lee, 2018), left-to-right generation of next sen\ntence words given a representation of the previous\n sentence (Kiros et al., 2015), or denoising auto\nencoder derived objectives (Hill et al., 2016).\n ELMo and its predecessor (Peters et al., 2017,\n 2018a) generalize traditional word embedding re\nsearch along a different dimension. They extract\n context-sensitive features from a left-to-right and a\n right-to-left language model. The contextual rep\nresentation of each token is the concatenation of\n the left-to-right and right-to-left representations.\n When integrating contextual word embeddings\n with existing task-specific architectures, ELMo\n advances the state of the art for several major NLP\n benchmarks (Peters et al., 2018a) including ques\ntion answering (Rajpurkar et al., 2016), sentiment\n analysis (Socher et al., 2013), and named entity\n recognition (Tjong Kim Sang and De Meulder,\n 2003). Melamud et al. (2016) proposed learning\n contextual representations through a task to pre\ndict a single word from both left and right context\n using LSTMs. Similar to ELMo, their model is\n feature-based and not deeply bidirectional. Fedus\n et al. (2018) shows that the cloze task can be used\n to improve the robustness of text generation mod\nels.\n 2.2 Unsupervised Fine-tuning Approaches\n As with the feature-based approaches, the first\n works in this direction only pre-trained word em\nbedding parameters from unlabeled text (Col\nlobert and Weston, 2008).\n More recently, sentence or document encoders\n which produce contextual token representations\n have been pre-trained from unlabeled text and\n f\n ine-tuned for a supervised downstream task (Dai\n and Le, 2015; Howard and Ruder, 2018; Radford\n et al., 2018). The advantage of these approaches\n is that few parameters need to be learned from\n scratch. At least partly due to this advantage,\n OpenAI GPT (Radford et al., 2018) achieved pre\nviously state-of-the-art results on many sentence\nlevel tasks from the GLUE benchmark (Wang\n et al., 2018a). Left-to-right language model\nNSP\n C\n Mask LM\n ...\n T1\n Mask LM\n T[SEP]\n TN\n T1\n ’ ...\n TM\n ’\n BERT\n MNLI\n NER\n SQuAD\n C\n ...\n T1\n Start/End Span\n T[SEP]\n TN\n T1\n ’ ...\n TM\n ’\n BERT\n BERT\n E[CLS]\n ...\n E1\n E[SEP]\n EN\n E1\n ’ ... EM\n ’\n [CLS]\n Tok 1\n Tok N\n ...\n [SEP]\n Masked Sentence A\n Masked Sentence B\n Unlabeled Sentence A and B Pair \nE[CLS]\n ...\n E1\n E[SEP]\n EN\n E1\n ’ ... EM\n ’\n Tok 1\n TokM\n ...\n [CLS]\n Tok 1\n Tok N\n ...\n Question\n [SEP]\n Tok 1\n TokM\n ...\n Paragraph\n Question Answer Pair\n Pre-training\n Fine-Tuning\n Figure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\ntures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\n models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\n symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\ntions/answers).\n ing and auto-encoder objectives have been used\n for pre-training such models (Howard and Ruder,\n 2018; Radford et al., 2018; Dai and Le, 2015).\n 2.3 Transfer Learning from Supervised Data\n There has also been work showing effective trans\nfer from supervised tasks with large datasets, such\n as natural language inference (Conneau et al.,\n 2017) and machine translation (McCann et al.,\n 2017). Computer vision research has also demon\nstrated the importance of transfer learning from\n large pre-trained models, where an effective recipe\n is to fine-tune models pre-trained with Ima\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n 3 BERT\n We introduce BERT and its detailed implementa\ntion in this section. There are two steps in our\n framework: pre-training and fine-tuning. Dur\ning pre-training, the model is trained on unlabeled\n data over different pre-training tasks. For fine\ntuning, the BERT model is first initialized with\n the pre-trained parameters, and all of the param\neters are fine-tuned using labeled data from the\n downstream tasks. Each downstream task has sep\narate fine-tuned models, even though they are ini\ntialized with the same pre-trained parameters. The\n question-answering example in Figure 1 will serve\n as a running example for this section.\n A distinctive feature of BERT is its unified ar\nchitecture across different tasks. There is mini\nmal difference between the pre-trained architec\nture and the final downstream architecture.\n Model Architecture BERT’s model architec\nture is a multi-layer bidirectional Transformer en\ncoder based on the original implementation de\nscribed in Vaswani et al. (2017) and released in\n the tensor2tensor library.1 Because the use\n of Transformers has become common and our im\nplementation is almost identical to the original,\n we will omit an exhaustive background descrip\ntion of the model architecture and refer readers to\n Vaswani et al. (2017) as well as excellent guides\n such as “The Annotated Transformer.”2\n In this work, we denote the number of layers\n (i.e., Transformer blocks) as L, the hidden size as\n H, and the number of self-attention heads as A.3\n We primarily report results on two model sizes:\n BERTBASE (L=12, H=768, A=12, Total Param\neters=110M) and BERTLARGE (L=24, H=1024,\n A=16, Total Parameters=340M).\n BERTBASE was chosen to have the same model\n size as OpenAI GPT for comparison purposes.\n Critically, however, the BERT Transformer uses\n bidirectional self-attention, while the GPT Trans\nformer uses constrained self-attention where every\n token can only attend to context to its left.4\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:45:18.011692Z","iopub.execute_input":"2025-01-09T10:45:18.011968Z","iopub.status.idle":"2025-01-09T10:45:18.019374Z","shell.execute_reply.started":"2025-01-09T10:45:18.011946Z","shell.execute_reply":"2025-01-09T10:45:18.018773Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"summary = summarize(input) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:45:22.727819Z","iopub.execute_input":"2025-01-09T10:45:22.728116Z","iopub.status.idle":"2025-01-09T10:46:04.126228Z","shell.execute_reply.started":"2025-01-09T10:45:22.728090Z","shell.execute_reply":"2025-01-09T10:46:04.125594Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:46:06.751922Z","iopub.execute_input":"2025-01-09T10:46:06.752198Z","iopub.status.idle":"2025-01-09T10:46:06.758087Z","shell.execute_reply.started":"2025-01-09T10:46:06.752176Z","shell.execute_reply":"2025-01-09T10:46:06.757163Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"' we introduce a new language representa \\ntion model called BERT, which stands for Bidirectional Encoder Representations from Transformers. BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers of the model. \\n the pre-trained model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. \\n we propose two approaches to apply pre-training language representations to downstream tasks: feature-based and finetuning based approaches.    in this paper \\n, we argue that current techniques restrict the \\n power of pre- trained representations by limiting the choice of archi  \\ntectures that can be used during pre- training. for example, in the present paper, the authors use a left-to-right architecture, where every token can only at\\ntend to previous tokens in the self-attention layers \\n of the Transformer (Vaswani et al., 2017). \\n such re \\nstrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying fine'"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"Humm Its working....","metadata":{}}]}