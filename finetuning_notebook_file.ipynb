{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10408342,"sourceType":"datasetVersion","datasetId":6449972}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## The notebook implementation of Research paper summarizer:\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install transformers datasets torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_name = \"facebook/bart-large-cnn\"\ntokenizer = AutoTokenizer.from_pretrained(model_name,clean_up_tokenization_spaces= True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:42:45.266647Z","iopub.execute_input":"2025-01-09T07:42:45.266949Z","iopub.status.idle":"2025-01-09T07:43:03.474207Z","shell.execute_reply.started":"2025-01-09T07:42:45.266925Z","shell.execute_reply":"2025-01-09T07:43:03.473579Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a38eeb722d94b188da0c09d75c15b13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da840393d7ee4f9c95eff5049fd89611"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4969e184741844a18eddb9da5a30788a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0088a4a20eee48b0b63a2fb5af33d671"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3076a4420b8e4f1fb405aeb6c3eb8b7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f7bfb62ccab4ff2b5eaf8f87e7b64e8"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:43:03.475476Z","iopub.execute_input":"2025-01-09T07:43:03.475802Z","iopub.status.idle":"2025-01-09T07:43:05.054883Z","shell.execute_reply.started":"2025-01-09T07:43:03.475783Z","shell.execute_reply":"2025-01-09T07:43:05.053840Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"dataset = load_dataset(\"scientific_papers\", \"arxiv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T01:24:18.023536Z","iopub.execute_input":"2025-01-09T01:24:18.023858Z","iopub.status.idle":"2025-01-09T01:29:29.139477Z","shell.execute_reply.started":"2025-01-09T01:24:18.023838Z","shell.execute_reply":"2025-01-09T01:29:29.137217Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e30a59ba6c5a43f085e31bd30f3288f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scientific_papers.py:   0%|          | 0.00/5.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bd8a880c35d4ed8885b47e7e1bbc83a"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for scientific_papers contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/scientific_papers.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.62G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b2a792db646443db31d1348cfe4c881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/880M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"532b4d30223d42e79fc16d2f2fa9f8bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/203037 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb17da788f444983bff1ae0f34077d3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/6436 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6424082b65d4123985abfd34963bdbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/6440 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba9c94800a874eb19e5950bc3f72f755"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"print(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T01:29:29.141256Z","iopub.execute_input":"2025-01-09T01:29:29.141515Z","iopub.status.idle":"2025-01-09T01:29:29.148058Z","shell.execute_reply.started":"2025-01-09T01:29:29.141491Z","shell.execute_reply":"2025-01-09T01:29:29.147412Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'abstract', 'section_names'],\n        num_rows: 203037\n    })\n    validation: Dataset({\n        features: ['article', 'abstract', 'section_names'],\n        num_rows: 6436\n    })\n    test: Dataset({\n        features: ['article', 'abstract', 'section_names'],\n        num_rows: 6440\n    })\n})\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def preprocess_func(text):\n    inputs = text['article']\n    targets= text['abstract']\n    model_inputs = tokenizer(inputs, max_length = 1024, truncation= True, padding= 'max_length')\n    labels = tokenizer(targets, max_length= 256, truncation= True, padding= 'max_length')\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset= dataset.map(preprocess_func, batched= True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T01:29:29.149099Z","iopub.execute_input":"2025-01-09T01:29:29.149416Z","iopub.status.idle":"2025-01-09T02:06:22.889440Z","shell.execute_reply.started":"2025-01-09T01:29:29.149383Z","shell.execute_reply":"2025-01-09T02:06:22.888519Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/203037 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d858a7e030fb4eb3b73b3a434f7060d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6436 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fdab53dd7c841b18d648398a9a9bbf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6440 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25861cc370bd479885c19c8d942baf6c"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# #@ Saving the mapped dataset so that it becomes effecient if training error occours \n\n# tokenized_dataset.save_to_disk(\"mapped_dataset_dir\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T02:14:07.310597Z","iopub.execute_input":"2025-01-09T02:14:07.310911Z","iopub.status.idle":"2025-01-09T02:14:52.910857Z","shell.execute_reply.started":"2025-01-09T02:14:07.310887Z","shell.execute_reply":"2025-01-09T02:14:52.909850Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/18 shards):   0%|          | 0/203037 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c1acd2999734ef98fd3a883f80ade2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6436 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dbbe15891b4403ab5cd10d1e31201b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6440 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36dcf989be71434f92c15f095735fab4"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# !zip -r compressed_mapped.zip mapped_dataset_dir","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Okay, now I'm not mapping it again. Instead, I'll unzip the mapped_value and run from this. ","metadata":{}},{"cell_type":"code","source":"from datasets import load_from_disk\n\nmapped_df= load_from_disk('/kaggle/input/compressed-mapped-files/mapped_dataset_dir')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:43:05.056432Z","iopub.execute_input":"2025-01-09T07:43:05.057135Z","iopub.status.idle":"2025-01-09T07:43:05.507070Z","shell.execute_reply.started":"2025-01-09T07:43:05.057097Z","shell.execute_reply":"2025-01-09T07:43:05.506088Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading dataset from disk:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1fde665a5fd4303ac5f50357df4331e"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args= TrainingArguments(\n    output_dir= \"./results\",\n    eval_strategy= \"epoch\", \n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    learning_rate= 2e-5, \n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    num_train_epochs= 3, \n    weight_decay = 0.01, \n    logging_dir=\"./logs\",\n    fp16 = True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:43:27.641419Z","iopub.execute_input":"2025-01-09T07:43:27.641884Z","iopub.status.idle":"2025-01-09T07:43:43.341128Z","shell.execute_reply.started":"2025-01-09T07:43:27.641842Z","shell.execute_reply":"2025-01-09T07:43:43.340349Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# small_train = tokenized_dataset[\"train\"].select(range(10000))\n# small_eval = tokenized_dataset[\"validation\"].select(range(4000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T02:07:13.240529Z","iopub.execute_input":"2025-01-09T02:07:13.240814Z","iopub.status.idle":"2025-01-09T02:07:13.249618Z","shell.execute_reply.started":"2025-01-09T02:07:13.240793Z","shell.execute_reply":"2025-01-09T02:07:13.248926Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"small_train = mapped_df[\"train\"].select(range(10000))\nsmall_eval = mapped_df[\"validation\"].select(range(2000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:43:43.342274Z","iopub.execute_input":"2025-01-09T07:43:43.342905Z","iopub.status.idle":"2025-01-09T07:43:43.352164Z","shell.execute_reply.started":"2025-01-09T07:43:43.342869Z","shell.execute_reply":"2025-01-09T07:43:43.351311Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train,\n    eval_dataset=small_eval,\n    tokenizer=tokenizer,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:44:14.800623Z","iopub.execute_input":"2025-01-09T07:44:14.800951Z","iopub.status.idle":"2025-01-09T07:44:17.646083Z","shell.execute_reply.started":"2025-01-09T07:44:14.800930Z","shell.execute_reply":"2025-01-09T07:44:17.645433Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_api_key\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:44:17.647240Z","iopub.execute_input":"2025-01-09T07:44:17.647562Z","iopub.status.idle":"2025-01-09T07:44:17.783366Z","shell.execute_reply.started":"2025-01-09T07:44:17.647531Z","shell.execute_reply":"2025-01-09T07:44:17.782757Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import wandb\nwandb.login(key= secret_value_0)\nwandb.init(project=\"Researchpaper_summarize\", name=\"wtf\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:44:18.557149Z","iopub.execute_input":"2025-01-09T07:44:18.557424Z","iopub.status.idle":"2025-01-09T07:44:31.502648Z","shell.execute_reply.started":"2025-01-09T07:44:18.557403Z","shell.execute_reply":"2025-01-09T07:44:31.501832Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfirojpaudel\u001b[0m (\u001b[33mfirojpaudel-madan-bhandari-memorial-college\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250109_074425-k0weyuq7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize/runs/k0weyuq7' target=\"_blank\">wtf</a></strong> to <a href='https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize' target=\"_blank\">https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize/runs/k0weyuq7' target=\"_blank\">https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize/runs/k0weyuq7</a>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/firojpaudel-madan-bhandari-memorial-college/Researchpaper_summarize/runs/k0weyuq7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7b57c5202320>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T07:44:31.503917Z","iopub.execute_input":"2025-01-09T07:44:31.504300Z","iopub.status.idle":"2025-01-09T10:31:28.710471Z","shell.execute_reply.started":"2025-01-09T07:44:31.504266Z","shell.execute_reply":"2025-01-09T10:31:28.709585Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 2:46:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.141400</td>\n      <td>1.818125</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.867500</td>\n      <td>1.777878</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.785800</td>\n      <td>1.769484</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1875, training_loss=1.8924777669270834, metrics={'train_runtime': 10015.5745, 'train_samples_per_second': 2.995, 'train_steps_per_second': 0.187, 'total_flos': 6.501313806336e+16, 'train_loss': 1.8924777669270834, 'epoch': 3.0})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"import shutil\nimport os\nlogs_dir = '/kaggle/working/results'\ntry:\n    if os.path.exists(logs_dir):\n        shutil.rmtree(logs_dir)  # Remove the directory and its contents\n        print(f\"Directory '{logs_dir}' removed successfully.\")\n    else:\n        print(f\"Directory '{logs_dir}' does not exist.\")\nexcept OSError as e:\n    print(f\"Error removing directory '{logs_dir}': {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:56:57.453442Z","iopub.execute_input":"2025-01-09T10:56:57.453817Z","iopub.status.idle":"2025-01-09T10:56:58.898804Z","shell.execute_reply.started":"2025-01-09T10:56:57.453788Z","shell.execute_reply":"2025-01-09T10:56:58.897834Z"}},"outputs":[{"name":"stdout","text":"Directory '/kaggle/working/results' removed successfully.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"##@ saving the finetuned model:\ntrainer.save_model(\"./fine_tuned_bart\")\ntokenizer.save_pretrained(\"./fine_tuned_bart\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:36:55.817087Z","iopub.execute_input":"2025-01-09T10:36:55.817442Z","iopub.status.idle":"2025-01-09T10:36:58.768844Z","shell.execute_reply.started":"2025-01-09T10:36:55.817415Z","shell.execute_reply":"2025-01-09T10:36:58.767959Z"}},"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('./fine_tuned_bart/tokenizer_config.json',\n './fine_tuned_bart/special_tokens_map.json',\n './fine_tuned_bart/vocab.json',\n './fine_tuned_bart/merges.txt',\n './fine_tuned_bart/added_tokens.json',\n './fine_tuned_bart/tokenizer.json')"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import os\n\nprint(os.listdir(\"./fine_tuned_bart\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:38:40.317897Z","iopub.execute_input":"2025-01-09T10:38:40.318223Z","iopub.status.idle":"2025-01-09T10:38:40.324335Z","shell.execute_reply.started":"2025-01-09T10:38:40.318195Z","shell.execute_reply":"2025-01-09T10:38:40.323563Z"}},"outputs":[{"name":"stdout","text":"['model.safetensors', 'training_args.bin', 'special_tokens_map.json', 'vocab.json', 'tokenizer_config.json', 'tokenizer.json', 'config.json', 'generation_config.json', 'merges.txt']\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"all the weights are there in model.safetensors and training arguments in training_args.bins","metadata":{}},{"cell_type":"code","source":"#@ Testing once before downloading the model \n\nmodel= AutoModelForSeq2SeqLM.from_pretrained('/kaggle/working/fine_tuned_bart')\ntokenizer= AutoTokenizer.from_pretrained('/kaggle/working/fine_tuned_bart')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:45:14.919999Z","iopub.execute_input":"2025-01-09T10:45:14.920279Z","iopub.status.idle":"2025-01-09T10:45:15.937024Z","shell.execute_reply.started":"2025-01-09T10:45:14.920258Z","shell.execute_reply":"2025-01-09T10:45:15.936090Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def summarize(testing):\n    #@ Tokenizng the inp text to test in the finetuned model\n    inputs = tokenizer(testing, max_length= 1024, truncation= True, return_tensors='pt')\n\n    #@ Generating the summary\n    summary_enc = model.generate(inputs['input_ids'], max_length= 500, min_length= 250, length_penalty= 2.0, num_beams=4, early_stopping= True)\n\n    #@ Decoding the generated summary\n    summary_fin = tokenizer.decode(summary_enc[0], skip_special_tokens= True)\n\n    return summary_fin","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:45:15.938148Z","iopub.execute_input":"2025-01-09T10:45:15.938455Z","iopub.status.idle":"2025-01-09T10:45:15.943762Z","shell.execute_reply.started":"2025-01-09T10:45:15.938426Z","shell.execute_reply":"2025-01-09T10:45:15.943060Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"input = '''\n 1 Introduction\n Language model pre-training has been shown to\n be effective for improving many natural language\n processing tasks (Dai and Le, 2015; Peters et al.,\n 2018a; Radford et al., 2018; Howard and Ruder,\n 2018). These include sentence-level tasks such as\n natural language inference (Bowman et al., 2015;\n Williams et al., 2018) and paraphrasing (Dolan\n and Brockett, 2005), which aim to predict the re\nlationships between sentences by analyzing them\n holistically, as well as token-level tasks such as\n named entity recognition and question answering,\n where models are required to produce fine-grained\n output at the token level (Tjong Kim Sang and\n De Meulder, 2003; Rajpurkar et al., 2016).\n There are two existing strategies for apply\ning pre-trained language representations to down\nstream tasks: feature-based and fine-tuning. The\n feature-based approach, such as ELMo (Peters\n et al., 2018a), uses task-specific architectures that\n include the pre-trained representations as addi\ntional features. The fine-tuning approach, such as\n the Generative Pre-trained Transformer (OpenAI\n GPT) (Radford et al., 2018), introduces minimal\n task-specific parameters, and is trained on the\n downstream tasks by simply fine-tuning all pre\ntrained parameters. The two approaches share the\n sameobjective function during pre-training, where\n they use unidirectional language models to learn\n general language representations.\n We argue that current techniques restrict the\n power of the pre-trained representations, espe\ncially for the fine-tuning approaches. The ma\njor limitation is that standard language models are\n unidirectional, and this limits the choice of archi\ntectures that can be used during pre-training. For\n example, in OpenAIGPT,theauthors use aleft-to\nright architecture, where every token can only at\ntend to previous tokens in the self-attention layers\n of the Transformer (Vaswani et al., 2017). Such re\nstrictions are sub-optimal for sentence-level tasks,\n and could be very harmful when applying fine\ntuning based approaches to token-level tasks such\n as question answering, where it is crucial to incor\nporate context from both directions.\n In this paper, we improve the fine-tuning based\n approaches by proposing BERT: Bidirectional\n Encoder Representations from Transformers.\n BERT alleviates the previously mentioned unidi\nrectionality constraint by using a “masked lan\nguage model” (MLM) pre-training objective, in\nspired by the Cloze task (Taylor, 1953). The\n masked language model randomly masks some of\n the tokens from the input, and the objective is to\n predict the original vocabulary id of the masked\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:51:39.950291Z","iopub.execute_input":"2025-01-09T10:51:39.950675Z","iopub.status.idle":"2025-01-09T10:51:39.956294Z","shell.execute_reply.started":"2025-01-09T10:51:39.950643Z","shell.execute_reply":"2025-01-09T10:51:39.955306Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"summary = summarize(input) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:51:40.929451Z","iopub.execute_input":"2025-01-09T10:51:40.929789Z","iopub.status.idle":"2025-01-09T10:52:20.905417Z","shell.execute_reply.started":"2025-01-09T10:51:40.929763Z","shell.execute_reply":"2025-01-09T10:52:20.904335Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:52:20.906643Z","iopub.execute_input":"2025-01-09T10:52:20.906952Z","iopub.status.idle":"2025-01-09T10:52:20.912364Z","shell.execute_reply.started":"2025-01-09T10:52:20.906920Z","shell.execute_reply":"2025-01-09T10:52:20.911700Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"' language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., \\n 2018a; Radford et al. \\n, 2018b; Howard and Ruder, 2018c ). \\n these include sentence-level tasks such as natural language inference and paraphrasing, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and \\n De Meulder, 2003; Rajpurkar et al,. 2016). \\n there are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The two approaches share the sameobjective function during pre- training, where they use unidirectional language models to learn general language representations (e.g. the Generative Pre-trained Transformer (OpenAI \\n GPT) and ELMo (Peters et al, 2018a) ). in this paper \\n we argue that current techniques restrict the power of the pre- trained representations by limiting the choice of archi \\n tectures that can be used during pre training, and we propose a new approach called Bidirectional Encoder Representations from Transformers (BERT ) that alleviates the previously mentioned unidi \\nrectionality constraint by using a “masked lan \\nguage model” (MLM) pre'"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"Humm Its working....","metadata":{}},{"cell_type":"code","source":"##@ Zipping the fine-tuned folder in order to download...\n!zip -r compressed_finttuned.zip /kaggle/working/fine_tuned_bart","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:57:13.223958Z","iopub.execute_input":"2025-01-09T10:57:13.224241Z","iopub.status.idle":"2025-01-09T10:58:38.514962Z","shell.execute_reply.started":"2025-01-09T10:57:13.224220Z","shell.execute_reply":"2025-01-09T10:58:38.514035Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/fine_tuned_bart/ (stored 0%)\n  adding: kaggle/working/fine_tuned_bart/model.safetensors (deflated 7%)\n  adding: kaggle/working/fine_tuned_bart/training_args.bin (deflated 51%)\n  adding: kaggle/working/fine_tuned_bart/special_tokens_map.json (deflated 52%)\n  adding: kaggle/working/fine_tuned_bart/vocab.json (deflated 59%)\n  adding: kaggle/working/fine_tuned_bart/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/fine_tuned_bart/tokenizer.json (deflated 72%)\n  adding: kaggle/working/fine_tuned_bart/config.json (deflated 61%)\n  adding: kaggle/working/fine_tuned_bart/generation_config.json (deflated 48%)\n  adding: kaggle/working/fine_tuned_bart/merges.txt (deflated 53%)\n","output_type":"stream"}],"execution_count":29}]}